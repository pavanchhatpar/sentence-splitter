{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentence splitting model.ipynb",
      "provenance": [],
      "mount_file_id": "19hHxrHJ3CFYLHzyjAC08qj1Qv4ByMuBm",
      "authorship_tag": "ABX9TyOaHRyUq+qrd8aYnBbQ4r40",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavanchhatpar/sentence-splitter/blob/master/Sentence_splitting_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8wLEVo517jt",
        "colab_type": "text"
      },
      "source": [
        "# Download and Extract data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYmwEAM05toN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gdown\n",
        "import tempfile\n",
        "import os\n",
        "import zipfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzk0n8vRuVt0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fileids = {\n",
        "    \"DiscoveryBase\": \"1bqyml-AD5EbisExKW5Vss4-Q_IhXy3lM\",\n",
        "    \"DiscoveryHard\": \"18NDudOIkSp86FtIEB9Wo3rIpYtxdGxI3\",\n",
        "    \"DiscoveryBig\": \"1OLgiGd3CCWIkROyTABS2APjplRiRJPoO\",\n",
        "}\n",
        "data_path = \"/content/data\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlAy6_RluePs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists(data_path):\n",
        "  os.makedirs(data_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0j5irzWY0dF8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "638d0a8c-797b-4229-8043-860321e2b6bb"
      },
      "source": [
        "for fileid in fileids.values():\n",
        "  url = f\"https://drive.google.com/uc?id={fileid}\"\n",
        "  with tempfile.NamedTemporaryFile() as f:\n",
        "    gdown.download(url, f.name, False)\n",
        "    z = zipfile.ZipFile(f)\n",
        "    z.extractall(data_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1bqyml-AD5EbisExKW5Vss4-Q_IhXy3lM\n",
            "To: /tmp/tmp7_i68lwc\n",
            "158MB [00:02, 59.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=18NDudOIkSp86FtIEB9Wo3rIpYtxdGxI3\n",
            "To: /tmp/tmp9xhpu1br\n",
            "158MB [00:02, 60.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1OLgiGd3CCWIkROyTABS2APjplRiRJPoO\n",
            "To: /tmp/tmp8wh6p3je\n",
            "316MB [00:05, 61.8MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNykCpX31_2Z",
        "colab_type": "text"
      },
      "source": [
        "# Process data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dz_k92Fu_FDj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCIdN6qi2GRb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfs = {}\n",
        "\n",
        "for dirname in fileids.keys():\n",
        "  dataset_path = os.path.join(data_path, dirname)\n",
        "  l_df=[]\n",
        "  for cv in [\"train\",\"test\",\"dev\"]:\n",
        "      df_cv=pd.DataFrame(list(zip(\n",
        "          open(f\"{dataset_path}/s1.{cv}\").read().splitlines(),\n",
        "          open(f\"{dataset_path}/s2.{cv}\").read().splitlines(),\n",
        "          open(f\"{dataset_path}/labels.{cv}\").read().splitlines()))\n",
        "          ,columns=[\"s1\",\"s2\",\"y\"])\n",
        "      df_cv[\"set\"]=cv\n",
        "      l_df+=[df_cv]\n",
        "      \n",
        "  dfs[dirname] = pd.concat(l_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_5AGwRq5uef",
        "colab_type": "text"
      },
      "source": [
        "### Join sentence 1 and sentence 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFOO-Y9x58al",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for df in dfs.values():\n",
        "  df.loc[:, \"full\"] = (\n",
        "      df.s1.str[:-1] # remove punctuation from end, but not the space chars\n",
        "      + df.y.str[:-1] # remove last ','\n",
        "      + \"  \" # double space for consistency with the existing space format\n",
        "      # + df.s2.str[:1].str.lower().str.cat(df.s2.str[1:]) # de-capitalize first letter\n",
        "      + df.s2 # Need not de-capitalize first letter because tokenizer used is case insensitive in this case\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcTfprrQ9cjz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "30635987-0611-4f96-a796-4d0b637fb78c"
      },
      "source": [
        "dfs[\"DiscoveryBase\"].head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>s1</th>\n",
              "      <th>s2</th>\n",
              "      <th>y</th>\n",
              "      <th>set</th>\n",
              "      <th>full</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>He  helped  to  found  the  Mexican  American ...</td>\n",
              "      <td>Sanchez  became  involved  with  the  American...</td>\n",
              "      <td>subsequently,</td>\n",
              "      <td>train</td>\n",
              "      <td>He  helped  to  found  the  Mexican  American ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Then  click  the  ``  Paper  Clip  ''  button ...</td>\n",
              "      <td>You  can  use  any  handheld  device  that  ru...</td>\n",
              "      <td>alternately,</td>\n",
              "      <td>train</td>\n",
              "      <td>Then  click  the  ``  Paper  Clip  ''  button ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>That  's  a  long  way  for  a  program  that ...</td>\n",
              "      <td>FAU  is  a  program  that  under  Cooney  and ...</td>\n",
              "      <td>presently,</td>\n",
              "      <td>train</td>\n",
              "      <td>That  's  a  long  way  for  a  program  that ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>FORT  DRUM  ,  N.Y.  -  Throughout  its  histo...</td>\n",
              "      <td>Soldiers  receive  assignments  to  multiple  ...</td>\n",
              "      <td>typically,</td>\n",
              "      <td>train</td>\n",
              "      <td>FORT  DRUM  ,  N.Y.  -  Throughout  its  histo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>They  continued  to  dig  noting  that  there ...</td>\n",
              "      <td>Every  ten  feet  they  found  a  layer  of  l...</td>\n",
              "      <td>curiously,</td>\n",
              "      <td>train</td>\n",
              "      <td>They  continued  to  dig  noting  that  there ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  s1  ...                                               full\n",
              "0  He  helped  to  found  the  Mexican  American ...  ...  He  helped  to  found  the  Mexican  American ...\n",
              "1  Then  click  the  ``  Paper  Clip  ''  button ...  ...  Then  click  the  ``  Paper  Clip  ''  button ...\n",
              "2  That  's  a  long  way  for  a  program  that ...  ...  That  's  a  long  way  for  a  program  that ...\n",
              "3  FORT  DRUM  ,  N.Y.  -  Throughout  its  histo...  ...  FORT  DRUM  ,  N.Y.  -  Throughout  its  histo...\n",
              "4  They  continued  to  dig  noting  that  there ...  ...  They  continued  to  dig  noting  that  there ...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-ykX3QH-B1w",
        "colab_type": "text"
      },
      "source": [
        "### Filter useful sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ne2PlKQT92oD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataparts = []\n",
        "for src, df in dfs.items():\n",
        "  newdf = {}\n",
        "  # If <s1> then, <s2>\n",
        "  subdf = df[(df.y == 'then,')&(df.s1.str.startswith('If ')&~(df.s1.str.contains(','))&~(df.s1.str.contains(' then ')))][['full', 'set']].to_dict(orient='list')\n",
        "  newdf['sentence'] = subdf['full']\n",
        "  newdf['set'] = subdf['set']\n",
        "\n",
        "  # s1 => \"If ... (then|,) ...\"\n",
        "  subdf = df[(df.s1.str.startswith('If '))&((df.s1.str.contains(','))|(df.s1.str.contains(' then ')))][['s1', 'set']].to_dict(orient='list')\n",
        "  newdf['sentence'].extend(subdf['s1'])\n",
        "  newdf['set'].extend(subdf['set'])\n",
        "\n",
        "  # s1 => \"If ... (then|,) ...\"\n",
        "  subdf = df[(df.s2.str.startswith('If '))&((df.s2.str.contains(','))|(df.s2.str.contains(' then ')))][['s2', 'set']].to_dict(orient='list')\n",
        "  newdf['sentence'].extend(subdf['s2'])\n",
        "  newdf['set'].extend(subdf['set'])\n",
        "\n",
        "  newdf = pd.DataFrame(newdf)\n",
        "  newdf['src'] = src\n",
        "  dataparts.append(newdf)\n",
        "\n",
        "data = pd.concat(dataparts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtfAuGAHIexi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "fae8dc48-b998-4f10-c732-22eae0fe2cef"
      },
      "source": [
        "print(data.shape)\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(203725, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>set</th>\n",
              "      <th>src</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>If  your  site  is  titled  ``  post  generati...</td>\n",
              "      <td>train</td>\n",
              "      <td>DiscoveryBase</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>If  you  want  to  compete  then  See  where  ...</td>\n",
              "      <td>train</td>\n",
              "      <td>DiscoveryBase</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>If  you  have  time  for  two  things  in  Sav...</td>\n",
              "      <td>train</td>\n",
              "      <td>DiscoveryBase</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>If  you  're  having  crash  issues  -  delete...</td>\n",
              "      <td>train</td>\n",
              "      <td>DiscoveryBase</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>If  a  driver  made  one  small  mistake  I  f...</td>\n",
              "      <td>train</td>\n",
              "      <td>DiscoveryBase</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence    set            src\n",
              "0  If  your  site  is  titled  ``  post  generati...  train  DiscoveryBase\n",
              "1  If  you  want  to  compete  then  See  where  ...  train  DiscoveryBase\n",
              "2  If  you  have  time  for  two  things  in  Sav...  train  DiscoveryBase\n",
              "3  If  you  're  having  crash  issues  -  delete...  train  DiscoveryBase\n",
              "4  If  a  driver  made  one  small  mistake  I  f...  train  DiscoveryBase"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eN212Q44Imbw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "fd4c147c-f5f6-4244-bdb4-9c4af4846682"
      },
      "source": [
        "# Some sentences might repeat in all three data sources\n",
        "\n",
        "data = data[~data.sentence.duplicated()]\n",
        "print(data.shape)\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(177587, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>set</th>\n",
              "      <th>src</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>If  your  site  is  titled  ``  post  generati...</td>\n",
              "      <td>train</td>\n",
              "      <td>DiscoveryBase</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>If  you  want  to  compete  then  See  where  ...</td>\n",
              "      <td>train</td>\n",
              "      <td>DiscoveryBase</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>If  you  have  time  for  two  things  in  Sav...</td>\n",
              "      <td>train</td>\n",
              "      <td>DiscoveryBase</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>If  you  're  having  crash  issues  -  delete...</td>\n",
              "      <td>train</td>\n",
              "      <td>DiscoveryBase</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>If  a  driver  made  one  small  mistake  I  f...</td>\n",
              "      <td>train</td>\n",
              "      <td>DiscoveryBase</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence    set            src\n",
              "0  If  your  site  is  titled  ``  post  generati...  train  DiscoveryBase\n",
              "1  If  you  want  to  compete  then  See  where  ...  train  DiscoveryBase\n",
              "2  If  you  have  time  for  two  things  in  Sav...  train  DiscoveryBase\n",
              "3  If  you  're  having  crash  issues  -  delete...  train  DiscoveryBase\n",
              "4  If  a  driver  made  one  small  mistake  I  f...  train  DiscoveryBase"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kYToi3sN6e0",
        "colab_type": "text"
      },
      "source": [
        "### Tokenize data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-Ti78QZMT3N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "0d26e07a-dc72-4adc-8e47-3215ac64df17"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/35/1c3f6e62d81f5f0daff1384e6d5e6c5758682a8357ebc765ece2b9def62b/transformers-3.0.0-py3-none-any.whl (754kB)\n",
            "\u001b[K     |████████████████████████████████| 757kB 17.1MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.0-rc4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/bd/e5abec46af977c8a1375c1dca7cb1e5b3ec392ef279067af7f6bc50491a0/tokenizers-0.8.0rc4-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 45.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 55.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 28.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=4d4772d022e315b8bd15dd5b7ecd1c6ad851c7b7b2275dc4cec9c49c8fcefbd1\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.0rc4 transformers-3.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbxEPpYAMY1J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import ElectraTokenizerFast"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HN9GHf9Ms6K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = ElectraTokenizerFast.from_pretrained(\"google/electra-base-discriminator\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fi1eeNFBQ5Hu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['tokenized'] = data.sentence.map(tokenizer.tokenize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sczt3O16SQS4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "41e248e2-f469-4b38-d0a6-d15592bf7557"
      },
      "source": [
        "data.tokenized.map(lambda l : len(l)).describe(percentiles=[.25, .5, .75, .95])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    177587.00000\n",
              "mean         25.38919\n",
              "std           7.51887\n",
              "min           5.00000\n",
              "25%          20.00000\n",
              "50%          25.00000\n",
              "75%          31.00000\n",
              "95%          38.00000\n",
              "max          73.00000\n",
              "Name: tokenized, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sn6vBbHnUCnu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_len = 50  # roughly between 95% to 100% lengths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMsukaAHUcs9",
        "colab_type": "text"
      },
      "source": [
        "### Find first 'then' or ',' in tokenized data - target token (split_pos)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fjsf38TTUkSL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['split_pos'] = data.tokenized.map(lambda l: l.index('then')+1 if 'then' in l else l.index(',')+1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ta_7INSd2Mo",
        "colab_type": "text"
      },
      "source": [
        "### Remove the connective token at split_pos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHwGcUW4cdEF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['tokenized'] = data.apply(lambda row: row.tokenized[:row.split_pos-1] + row.tokenized[row.split_pos:], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a641GNGPfhDW",
        "colab_type": "text"
      },
      "source": [
        "### Encode tokens to IDs thus getting final features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fk16FiuEpvkn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "f097d425-2f26-40bd-b090-1cdda25b8771"
      },
      "source": [
        "data.set.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "train    159907\n",
              "test       8928\n",
              "dev        8752\n",
              "Name: set, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9BtdKGWprNm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sets = [\n",
        "      \"train\",\n",
        "      \"test\",\n",
        "      \"dev\"\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCq7_Qt8p5RT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features = {}\n",
        "for split in sets:\n",
        "  subset = data[data.set == split]\n",
        "  features[split] = tokenizer(list(subset.tokenized.values), is_pretokenized=True, max_length=max_len, padding=True, truncation=True)\n",
        "  features[split]['split_position'] = list(subset.split_pos.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZMFz3nwj4dI",
        "colab_type": "text"
      },
      "source": [
        "### Convert to TF Dataset for train, test and validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cx-nW0dbl_AR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from functools import partial"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gd_kheGYkgRx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gen(split, allfeatures):\n",
        "  features = allfeatures[split]\n",
        "  for input_id, token_type_id, attention_mask, split_position in zip(features['input_ids'], features['token_type_ids'], features['attention_mask'], features['split_position']):\n",
        "    yield ({\n",
        "        \"input_ids\": input_id,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"token_type_ids\": token_type_id,},\n",
        "        split_position)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O7DJYD-qnAp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf_datasets = {}\n",
        "for split in sets:\n",
        "  dataset = tf.data.Dataset.from_generator(\n",
        "      partial(gen, split=split, allfeatures=features),\n",
        "      ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int32),\n",
        "      ({\"input_ids\": tf.TensorShape([None]), \"attention_mask\": tf.TensorShape([None]), \"token_type_ids\": tf.TensorShape([None])}, tf.TensorShape([]))\n",
        "  )\n",
        "  tf_datasets[split] = dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kXNH-ZilaBE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "2b849a24-4181-47f5-d19a-926c42220aad"
      },
      "source": [
        "tf_datasets['train'].element_spec"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'attention_mask': TensorSpec(shape=(None,), dtype=tf.int32, name=None),\n",
              "  'input_ids': TensorSpec(shape=(None,), dtype=tf.int32, name=None),\n",
              "  'token_type_ids': TensorSpec(shape=(None,), dtype=tf.int32, name=None)},\n",
              " TensorSpec(shape=(), dtype=tf.int32, name=None))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwnoB5u8qe9n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "6ca39ce1-73aa-4ba7-b62a-abde1741ee52"
      },
      "source": [
        "next(tf_datasets['train'].as_numpy_iterator())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1], dtype=int32),\n",
              "  'input_ids': array([  101,  2065,  2115,  2609,  2003,  4159,  1036,  1036,  2695,\n",
              "          4245,  1060, 11265,  1001,  1001, 16428,  2015,  1057,  1011,\n",
              "          9152,  1001,  1001,  8915,   999,  1005,  2092,  1010,  2017,\n",
              "          2113,  1010,  3071,  2097,  2022,  2559,  2012,  1996,  3931,\n",
              "          1999,  2543,  1001,  1001,  1042,  2080,  1001,  1001,  1060,\n",
              "          2012, 14883,  1001,  1001,   102], dtype=int32),\n",
              "  'token_type_ids': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0], dtype=int32)},\n",
              " 19)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfVpXlw1rUYm",
        "colab_type": "text"
      },
      "source": [
        "### Save processed datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46AjiOrIrYFu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_example(X, y):\n",
        "    serialized = tf.py_function(\n",
        "        serialize,\n",
        "        [X['input_ids'], X['attention_mask'], X['token_type_ids'], y],\n",
        "        tf.string\n",
        "    )\n",
        "    return tf.reshape(serialized, ())\n",
        "\n",
        "def bytes_feature(value):\n",
        "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
        "\n",
        "    # BytesList won't unpack a string from an EagerTensor.\n",
        "    if isinstance(value, type(tf.constant(0))):\n",
        "        value = value.numpy()\n",
        "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "\n",
        "def serialize(input_ids, attention_mask, token_type_ids, y):\n",
        "    input_ids = tf.io.serialize_tensor(input_ids)\n",
        "    attention_mask = tf.io.serialize_tensor(attention_mask)\n",
        "    token_type_ids = tf.io.serialize_tensor(token_type_ids)\n",
        "    label = tf.io.serialize_tensor(y)\n",
        "    feature = {\n",
        "        \"input_ids\": bytes_feature(input_ids),\n",
        "        \"attention_mask\": bytes_feature(attention_mask),\n",
        "        \"token_type_ids\": bytes_feature(token_type_ids),\n",
        "        \"label\": bytes_feature(label),\n",
        "    }\n",
        "    example_proto = tf.train.Example(\n",
        "        features=tf.train.Features(feature=feature))\n",
        "    return example_proto.SerializeToString()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlOPZ09cs15t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = tf_datasets['train'].map(make_example)\n",
        "test = tf_datasets['test'].map(make_example)\n",
        "dev = tf_datasets['dev'].map(make_example)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVU3apmbs6c8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save(train, test, dev, location):\n",
        "    if not os.path.exists(location):\n",
        "        os.makedirs(location)\n",
        "    if not os.path.isdir(location):\n",
        "        raise ValueError(f\"{location} should be a directory!\")\n",
        "    print(\"******** Saving Dev set ********\")\n",
        "    fname = os.path.join(location, \"dev.tfrecord\")\n",
        "    writer = tf.data.experimental.TFRecordWriter(fname, \"ZLIB\")\n",
        "    writer.write(dev)\n",
        "\n",
        "    print(\"******** Saving Test set ********\")\n",
        "    fname = os.path.join(location, \"test.tfrecord\")\n",
        "    writer = tf.data.experimental.TFRecordWriter(fname, \"ZLIB\")\n",
        "    writer.write(test)\n",
        "\n",
        "    print(\"******** Saving Training set ********\")\n",
        "    fname = os.path.join(location, \"train.tfrecord\")\n",
        "    writer = tf.data.experimental.TFRecordWriter(fname, \"ZLIB\")\n",
        "    writer.write(train)\n",
        "    print(\"******** Finished saving dataset ********\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LdSiDlwt7tD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "409dc253-e603-4820-f478-2cab5904f383"
      },
      "source": [
        "save(train, test, dev, \"./processed_data\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "******** Saving Dev set ********\n",
            "******** Saving Test set ********\n",
            "******** Saving Training set ********\n",
            "******** Finished saving dataset ********\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rm8A-6KnuAyz",
        "colab_type": "text"
      },
      "source": [
        "### Load saved dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AV1o112zuMW8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_ex(example_proto):\n",
        "    feature_description = {\n",
        "        'input_ids': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
        "        'attention_mask': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
        "        'token_type_ids': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
        "        'label': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
        "    }\n",
        "    example = tf.io.parse_single_example(\n",
        "        example_proto, feature_description)\n",
        "    input_ids = tf.io.parse_tensor(example['input_ids'], out_type=tf.int32)\n",
        "    input_ids.set_shape([None, ])\n",
        "    attention_mask = tf.io.parse_tensor(example['attention_mask'], out_type=tf.int32)\n",
        "    attention_mask.set_shape([None, ])\n",
        "    token_type_ids = tf.io.parse_tensor(example['token_type_ids'], out_type=tf.int32)\n",
        "    token_type_ids.set_shape([None, ])\n",
        "    label = tf.io.parse_tensor(example['label'], out_type=tf.int32)\n",
        "    label.set_shape([])\n",
        "    return ({\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"token_type_ids\": token_type_ids}, label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYHQPBvxvFR_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load(location):\n",
        "  train = os.path.join(location, \"train.tfrecord\")\n",
        "  dev = os.path.join(location, \"dev.tfrecord\")\n",
        "  test = os.path.join(location, \"test.tfrecord\")\n",
        "\n",
        "  train = tf.data.TFRecordDataset([train], compression_type='ZLIB')\n",
        "  dev = tf.data.TFRecordDataset([dev], compression_type='ZLIB')\n",
        "  test = tf.data.TFRecordDataset([test], compression_type='ZLIB')\n",
        "\n",
        "  AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "  train = train.map(parse_ex, num_parallel_calls=AUTOTUNE)\n",
        "  dev = dev.map(parse_ex, num_parallel_calls=AUTOTUNE)\n",
        "  test = test.map(parse_ex, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "  return train, test, dev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-RpJtEivplD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, test, dev = load(\"./processed_data\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ylB0I7ZxXPc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "9b0f91ec-58e8-4352-e7ca-858c646a6477"
      },
      "source": [
        "train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<ParallelMapDataset shapes: ({input_ids: (None,), attention_mask: (None,), token_type_ids: (None,)}, ()), types: ({input_ids: tf.int32, attention_mask: tf.int32, token_type_ids: tf.int32}, tf.int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OScO2_wSkE_I",
        "colab_type": "text"
      },
      "source": [
        "# Modeling a sentence splitting network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncFFZlKSkTdc",
        "colab_type": "text"
      },
      "source": [
        "### ELECTRA\n",
        " - Proposes a new method to train a language model borrowing concepts from GAN\n",
        " - It does have a generator and discriminator but it is not training in an adversarial fashion, uses just the maximum likelihood loss\n",
        " - The generator is a Masked LM and the discriminator predicts if a token in the sequence is real/ replaced.\n",
        " - Discriminator becomes the final LM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNmUwxdplCrL",
        "colab_type": "text"
      },
      "source": [
        "### Sentence Splitting Head\n",
        " - We use the pretrained ELECTRA model and replace its binary real/replaced final layer with a softmax layer of vocab_size\n",
        " - ELECTRA is just a choice, other pretrained LMs can also be used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LikFBCRBA6St",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import TFElectraPreTrainedModel\n",
        "from transformers.modeling_tf_electra import TFElectraMainLayer\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PgaWbD8BYFa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TFSentenceSplitLoss:\n",
        "  def compute_loss(self, labels, logits):\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "      from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n",
        "    split_loss = loss_fn(labels, logits)\n",
        "    print(f\"Loss {split_loss}\")\n",
        "    return split_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdKLX05E--oi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TFElectraForSentenceSplitting(TFElectraPreTrainedModel, TFSentenceSplitLoss):\n",
        "  def __init__(self, config, *inputs, **kwargs):\n",
        "    super(TFElectraForSentenceSplitting, self).__init__(config, *inputs, **kwargs)\n",
        "    self.electra = TFElectraMainLayer(config, name=\"electra\")\n",
        "    self.split_output = tf.keras.layers.Dense(\n",
        "        1,\n",
        "        name=\"split_output\")\n",
        "\n",
        "  def call(\n",
        "      self, \n",
        "      inputs=None,\n",
        "      attention_mask=None,\n",
        "      token_type_ids=None,\n",
        "      position_ids=None,\n",
        "      head_mask=None,\n",
        "      inputs_embeds=None,\n",
        "      output_attentions=None,\n",
        "      output_hidden_states=None,\n",
        "      labels=None,\n",
        "      training=False):\n",
        "    discriminator_hidden_states = self.electra(\n",
        "        inputs=inputs,\n",
        "        attention_mask=attention_mask,\n",
        "        token_type_ids=token_type_ids,\n",
        "        position_ids=position_ids,\n",
        "        head_mask=head_mask,\n",
        "        inputs_embeds=inputs_embeds,\n",
        "        output_attentions=output_attentions,\n",
        "        output_hidden_states=output_hidden_states,\n",
        "        training=training,\n",
        "    )\n",
        "    discriminator_sequence_output = discriminator_hidden_states[0]\n",
        "\n",
        "    # (batch_size, seq_len, 1)\n",
        "    logits = self.split_output(discriminator_sequence_output)\n",
        "\n",
        "    # (batch_size, seq_len)\n",
        "    split_logits = tf.squeeze(logits, axis=-1)\n",
        "\n",
        "    outputs = (split_logits,) + discriminator_hidden_states[1:]\n",
        "\n",
        "    if labels is not None:\n",
        "      loss = self.compute_loss(labels, outputs[0])\n",
        "      outputs = (loss,) + outputs\n",
        "\n",
        "    return outputs  # (loss), split_logits, (hidden_states), (attentions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDy_0eMhlpd8",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2RzoptyJxO9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "b37255c8-6997-4e2d-dc4c-6f57a74f0479"
      },
      "source": [
        "model = TFElectraForSentenceSplitting.from_pretrained(\"google/electra-base-discriminator\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at google/electra-base-discriminator were not used when initializing TFElectraForSentenceSplitting: ['discriminator_predictions']\n",
            "- This IS expected if you are initializing TFElectraForSentenceSplitting from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing TFElectraForSentenceSplitting from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of TFElectraForSentenceSplitting were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['split_output']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7O0CRT-ntztj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=loss,\n",
        "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e_OCz9xyPZV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "8b52403f-cc2d-4404-a977-575ead89aa34"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Jul  2 14:56:22 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.36.06    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   68C    P0    32W /  70W |   9153MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXrN4tYKI8gB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# There are some long sentences where the connective goes beyond `max_len`\n",
        "# We need to remove them so that our training model doesn't crash\n",
        "\n",
        "train = train.filter(lambda X, y: y < 50)\n",
        "dev = dev.filter(lambda X, y: y < 50)\n",
        "test = test.filter(lambda X, y: y < 50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBtYEPOoszrC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "c4b5e9ec-e903-4e17-8584-e2f5ce541d27"
      },
      "source": [
        "batch_size = 32\n",
        "model.fit(train.batch(batch_size), epochs=2, validation_data=dev.batch(batch_size))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "4997/4997 [==============================] - 1820s 364ms/step - loss: 0.4541 - sparse_categorical_accuracy: 0.8611 - val_loss: 0.2914 - val_sparse_categorical_accuracy: 0.9150\n",
            "Epoch 2/2\n",
            "4997/4997 [==============================] - 1823s 365ms/step - loss: 0.2568 - sparse_categorical_accuracy: 0.9196 - val_loss: 0.2826 - val_sparse_categorical_accuracy: 0.9181\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa0601a58d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQXu7ecRi4gY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "93a86164-ca73-4c3c-8ee3-c7f38171bd18"
      },
      "source": [
        "model.evaluate(train.batch(batch_size))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4997/4997 [==============================] - 601s 120ms/step - loss: 0.1442 - sparse_categorical_accuracy: 0.9543\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.14421269297599792, 0.9543038606643677]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVfY9yvyS7gp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "77faa002-bf5f-40a8-b19b-0a8d9c58b573"
      },
      "source": [
        "model.evaluate(dev.batch(batch_size))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "274/274 [==============================] - 33s 120ms/step - loss: 0.2826 - sparse_categorical_accuracy: 0.9181\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.2826082408428192, 0.9180758595466614]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ap6nJ1-lgNl4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7e9ec118-a724-4ac2-c3ef-ca5816f16b4e"
      },
      "source": [
        "model.evaluate(test.batch(batch_size))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "279/279 [==============================] - 34s 121ms/step - loss: 0.2956 - sparse_categorical_accuracy: 0.9188\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.2956418991088867, 0.9187948107719421]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07n6k8s0l2Ei",
        "colab_type": "text"
      },
      "source": [
        "# Model pipeline\n",
        " - A very useful feature from the transformers library\n",
        " - Abstracts the entire text pre-processing and model prediction work into an end-to-end pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWQNJnG0AiyN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7_jfw_fBNAX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = pipeline(\"sentiment-analysis\", model, model.config, tokenizer, \"tf\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kns9NRDUBqvJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = next(dev.as_numpy_iterator())[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShQT-sLVCrZs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "11dd09a4-0423-4660-a00d-9f1feadf49be"
      },
      "source": [
        "tokenizer.vocab_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30522"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 195
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhBXTghSCzLE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJjBdn8xChR9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.config.id2label = dict(zip(np.arange(tokenizer.vocab_size), (np.arange(tokenizer.vocab_size) - 1).astype(str)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoOSKw39mTk6",
        "colab_type": "text"
      },
      "source": [
        "### Save pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1yXQYKjF5Y8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del model.config.id2label # not needed, unnecessary use of space"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYQiWto2Sdhc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2a1d55ff-b11a-427b-8dda-8f7c59f627b8"
      },
      "source": [
        "total_variables = 0\n",
        "for var in model.variables:\n",
        "  total_variables += tf.size(var)\n",
        "print(\"Total\", total_variables.numpy(), \"variables\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total 108892417 variables\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tk7bN0WgFFC4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp.save_pretrained(\"./saved_model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QEyN10Clrj-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "daf2e888-284d-4c85-8575-1d0f15c1e7dd"
      },
      "source": [
        "!ls -lh ./saved_model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 416M\n",
            "-rw-r--r-- 1 root root  585 Jul  2 17:32 config.json\n",
            "-rw-r--r-- 1 root root  112 Jul  2 17:32 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root 416M Jul  2 17:32 tf_model.h5\n",
            "-rw-r--r-- 1 root root   48 Jul  2 17:32 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root 227K Jul  2 17:32 vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wu8FYTyemfbJ",
        "colab_type": "text"
      },
      "source": [
        "### Load pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmWEfgWnKAmL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "a65009c5-e3ad-4954-9e10-e1b6de6256e0"
      },
      "source": [
        "model1 = TFElectraForSentenceSplitting.from_pretrained(\"./saved_model\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint weights were used when initializing TFElectraForSentenceSplitting.\n",
            "\n",
            "All the weights of TFElectraForSentenceSplitting were initialized from the model checkpoint at ./saved_model.\n",
            "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFElectraForSentenceSplitting for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjaddbVOl_xl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer1 = ElectraTokenizerFast.from_pretrained(\"./saved_model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4V4JqRQNKpOG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1.config.id2label = dict(zip(np.arange(tokenizer1.vocab_size), np.arange(tokenizer1.vocab_size) - 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68e327A-GHen",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp1 = pipeline(\"sentiment-analysis\", model1, model1.config, tokenizer1, 'tf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_WbiOXKmpMC",
        "colab_type": "text"
      },
      "source": [
        "# Other ways to train - not working with TF 2.2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbHWPj2Lu784",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import TFTrainer, TFTrainingArguments"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSD00v3cuuOc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_args = TFTrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=2,              # total # of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        ")\n",
        "\n",
        "trainer = TFTrainer(\n",
        "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train.batch(32),    # tensorflow_datasets training dataset\n",
        "    eval_dataset=dev.batch(32)       # tensorflow_datasets evaluation dataset\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WG_PqzhXvIkE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0e9f4283-f2c5-4d7c-a18b-fbf644e66ed3"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/transformers/trainer_tf.py:440: StrategyBase.experimental_run_v2 (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "renamed to `run`\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/trainer_tf.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch_iter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs_trained\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_loss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_training_steps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_logging\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_iter\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/trainer_tf.py\u001b[0m in \u001b[0;36m_training_steps\u001b[0;34m(self, ds, optimizer)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0mReturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mgenerator\u001b[0m \u001b[0mover\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         \"\"\"\n\u001b[0;32m--> 401\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accumulate_next_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/trainer_tf.py\u001b[0m in \u001b[0;36m_accumulate_next_gradients\u001b[0;34m(self, ds)\u001b[0m\n\u001b[1;32m    432\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0m_accumulate_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    625\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    504\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    505\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 506\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2444\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2445\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2446\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2447\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2777\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2665\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2666\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2667\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2668\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2669\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/transformers/trainer_tf.py:430 _accumulate_next  *\n        return self._accumulate_gradients(per_replica_features, per_replica_labels)\n    /usr/local/lib/python3.6/dist-packages/transformers/trainer_tf.py:440 _accumulate_gradients  *\n        per_replica_loss = self.args.strategy.experimental_run_v2(\n    /usr/local/lib/python3.6/dist-packages/transformers/trainer_tf.py:453 _forward  *\n        per_example_loss, _ = self._run_model(features, labels, True)\n    /usr/local/lib/python3.6/dist-packages/transformers/trainer_tf.py:474 _run_model  *\n        loss, logits = self.model(features, labels=labels, training=training)[:2]\n    <ipython-input-11-d1e99ec5e088>:21 call  *\n        discriminator_hidden_states = self.electra(\n    /usr/local/lib/python3.6/dist-packages/transformers/modeling_tf_electra.py:292 call  *\n        hidden_states = self.embeddings([input_ids, position_ids, token_type_ids, inputs_embeds], training=training)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:968 __call__  **\n        outputs = self.call(cast_inputs, *args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/transformers/modeling_tf_electra.py:90 call\n        return self._embedding(inputs, training=training)\n    /usr/local/lib/python3.6/dist-packages/transformers/modeling_tf_electra.py:117 _embedding\n        embeddings = self.LayerNorm(embeddings)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:927 __call__\n        outputs = call_fn(cast_inputs, *args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/normalization.py:1133 call\n        scale, offset = _broadcast(self.gamma), _broadcast(self.beta)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/normalization.py:1120 _broadcast\n        return array_ops.reshape(v, broadcast_shape)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py:193 reshape\n        result = gen_array_ops.reshape(tensor, shape, name)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py:8087 reshape\n        \"Reshape\", tensor=tensor, shape=shape, name=name)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:473 _apply_op_helper\n        raise err\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:470 _apply_op_helper\n        preferred_dtype=default_dtype)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1341 convert_to_tensor\n        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py:321 _constant_tensor_conversion_function\n        return constant(v, dtype=dtype, name=name)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py:262 constant\n        allow_broadcast=True)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py:300 _constant_impl\n        allow_broadcast=allow_broadcast))\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:547 make_tensor_proto\n        \"supported type.\" % (type(values), values))\n\n    TypeError: Failed to convert object of type <class 'list'> to Tensor. Contents: [1, 1, None, 1]. Consider casting elements to a supported type.\n"
          ]
        }
      ]
    }
  ]
}